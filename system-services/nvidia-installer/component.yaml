version: 1.5.30
description: system pod for installing nvidia gpu drivers in the host operating system

ignoreVersion:
- 184.0.0-450.80.02-sap2
- 184.0.0-450.80.02-sap3
- 318.8.0-440.82-sap1
- 318.8.0-450.80.02-sap1
- 318.8.0-470.82.01-sap1
- 318.8.0-sap2
- 318.8.0-sap3

context:
  gid: com.sap.ai
  nvidiaDriverVersion:
#  - '440.82'
#  - 450.80.02
  - 470.82.01

  gardenLinux:
#    '184':
#      version: 184.0.0
#      debianBaseImageTag: bullseye-20200224-slim
#    '318':
#      version: 318.8.0
#      debianBaseImageTag: bullseye-20200224-slim
#    '576.1':
#      version: 576.1.0
#      debianBaseImageTag: bullseye-20200224-slim
    '576.7':
      version: 576.7.0
      debianBaseImageTag: bullseye-20200224-slim
    '576.10':
      version: 576.10.0
      debianBaseImageTag: bullseye-20200224-slim


steps:
  jenkins:
    type: jenkinsJob
    job: AI-Foundation/berlin-jenkins/AI-Core/nvidia-installer
    params:
      ComponentName: '{builtin.component_folder_basename}'
      BuildContext: '{builtin.encoded_contexts}'
      RunStaticScans: '{RunStaticScans}'
      RunUnitTests: '{RunUnitTests}'
      RunTests: '{RunTests}'
      RunCheckmarx: '{RunCheckmarx}'
      RunWhitesource: '{RunWhitesource}'
      RunSonarQube: '{RunSonarQube}'
      Release: '{Release}'
      Build: '{Build}'
      BuildXmake: '{BuildXmake}'
      TREEISH: '{builtin.git_commit}'
      RunWhitesourceDocker: '{RunWhitesourceDocker}'
      RunPPMSComplianceForImage: '{RunPPMSComplianceForImage}'
      RunProtecode: '{RunProtecode}'
  build:
    type: parameterized
    parameters:
    - nvidiaDriverVersion: '{nvidiaDriverVersion}'
    - gardenLinux: '{gardenLinux}'
    step:
      type: dockerbuild
      imageName: com.sap.ai/nvidia-installer-{gardenLinux.version}-{nvidiaDriverVersion}{image_suffix}
      registry: '{docker_registry}'
      dockerfile: Dockerfile
      buildArgs:
        DRIVER_VERSION: '{nvidiaDriverVersion}'
        GARDENLINUX_VERSION: '{gardenLinux.version}'
        DEBIAN_BASE_IMAGE_TAG: '{gardenLinux.debianBaseImageTag}'
        GIT_COMMIT: '{builtin.git_commit}'
        GIT_TAG: '{builtin.version}'
        GIT_TREE_STATE: '{builtin.git_tree_state}'
  lint:
    type: dockerRun
    dockerfile: test.Dockerfile
    shell: /bin/bash
    env:
      ARTIFACTS: /repo
    commands:
    - helm lint helm --set targetInfrastructure.gardener=true > "${{ARTIFACTS}}"/helm-lint.out
    - lint_exit_code=$?
    - cat "${{ARTIFACTS}}"/helm-lint.out
    - echo \"helm lint exit code was ${{lint_exit_code}}\"
    - if [[ ${{lint_exit_code}} != 0 ]]; then exit ${{lint_exit_code}}; fi

    - shellcheck resources/* > "${{ARTIFACTS}}"/shellcheck.out
    - shellcheck_exit_code=$?
    - cat "${{ARTIFACTS}}"/shellcheck.out
    - echo \"shellcheck exit code was ${{shellcheck_exit_code}}\"
    - exit ${{shellcheck_exit_code}}
  unit:
    type: dockerRun
    dockerfile: test.Dockerfile
    shell: /bin/bash
    env:
      ARTIFACTS: /repo
    commands:
    - helm template nvidia-installer helm --set targetInfrastructure.gardener=true
      > "${{ARTIFACTS}}"/nvidia-installer.yaml
    - template_exit_code=$?
    - cat "${{ARTIFACTS}}"/nvidia-installer.yaml
    - echo \"helm template exit code was ${{template_exit_code}}\"
    - if [[ ${{template_exit_code}} != 0 ]]; then exit ${{template_exit_code}}; fi

    - kubeconform --kubernetes-version 1.21.0 --strict --summary --verbose "${{ARTIFACTS}}"/nvidia-installer.yaml
      > "${{ARTIFACTS}}"/kubeconform.out
    - kubeconform_exit_code=$?
    - cat "${{ARTIFACTS}}"/kubeconform.out
    - echo \"kubeconform exit code was ${{kubeconform_exit_code}}\"
    - exit ${{kubeconform_exit_code}}
  whitesource:
    type: whitesource

pipelines:
  local:
    build: build
  push-validation:
    build: jenkins
  main:
    build: jenkins
  release:
    build: jenkins

patch:
- file: .xmake.cfg
  regexp: version=.*
  replace: version={builtin.version}
- file: helm/Chart.yaml
  yaml_path: version
  replace: '{builtin.version}'
- file: helm/values.yaml
  regexp: 'installerTag: .*'
  replace: 'installerTag: {builtin.version}'
- file: wss-unified-agent.config
  regexp: projectVersion=.*
  replace: projectVersion={builtin.component_version}

dependencies:
- component: foss_images/kubectl
  patch:
  - file: helm/values.yaml
    regexp: '  kubectlImage: .*'
    replace: '  kubectlImage: {imageName}:{builtin.version}'
- component: foss_images/pause
  patch:
  - file: helm/values.yaml
    regexp: '  pauseImage: .*'
    replace: '  pauseImage: {imageName}:{builtin.version}'
- component: foss_images/nvidia-gpu-device-plugin
  patch:
  - file: helm/values.yaml
    regexp: '  devicePluginImage: .*'
    replace: '  devicePluginImage: {imageName}:{builtin.version}'
